{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJf2QEzDlXGeXJlBpf2JzH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The downside of tf-idf / bow is it dosent understand lthe semantics of the tokens into the account. Example, Football == Soccer(sports) & Tiger == Lion(animals). So in word embeddings, words with similar meanings, have a similar representation. There are 2 type of word embeddings, one is context-dependents(RNN, LSTM) and another is context-independent (Word2Vec, Glove).\n",
        "\n",
        "Context-dependent and Transformer-based (future topic) - (Transformers, BERT, XLM. ROBERTa, etc)"
      ],
      "metadata": {
        "id": "LEc4iqa0Af2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wod1kCOf8tvF"
      },
      "outputs": [],
      "source": [
        "#word2vec\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Imagine we have a collection of sentences\n",
        "sentences = [\n",
        "    \"I love playing with my dog\",\n",
        "    \"Cats are cute\",\n",
        "    \"Dogs are loyal\",\n",
        "    \"I enjoy running in the park\"\n",
        "]\n",
        "\n",
        "# First we convert these sentences into a list of words\n",
        "# We split each sentence into individual words and create a list of all the words\n",
        "word_list = [word for sentence in sentences for word in sentence.split()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we create a vocabulary of unique words\n",
        "# We remove duplicate words and assign each word a unique number\n",
        "vocabulary = list(set(word_list))\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "rJLP3YutGyAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can create our Word2Vec model\n",
        "embedding_size = 10  # Size of word embeddings (each word will be represented as a 10-dimensional vector)"
      ],
      "metadata": {
        "id": "y2KJMYx6GyL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We initialize the word vectors randomly\n",
        "word_vectors = np.random.randn(len(vocabulary), embedding_size)"
      ],
      "metadata": {
        "id": "rJlrdDMzGyOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word vectors before training:\")\n",
        "print(word_vectors)"
      ],
      "metadata": {
        "id": "rsYe_Vy7GyRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of skip-gram is to predict the context words given a target word. For example, if the target word is \"love,\" the context words could be \"I,\" \"playing,\" \"with,\" and \"my.\" Skip-gram learns the relationships between words by training on pairs of target words and their corresponding context words."
      ],
      "metadata": {
        "id": "GrG6DH2yIEIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use a technique called skip gram to train our Word2Vec model\n",
        "# Skip-gram tries to predict the context words given a target word (relationship)\n",
        "\n",
        "# Define a function to generate training data for our skip-gram model\n",
        "def generate_training_data(window_size):\n",
        "    training_data = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "\n",
        "        for i, target_word in enumerate(words):\n",
        "            for context_word in words[max(0, i - window_size): i] + words[i + 1: i + window_size + 1]:\n",
        "                training_data.append((target_word, context_word))\n",
        "\n",
        "    return training_data\n",
        "\n",
        "# Generate training data with a window size of 1\n",
        "window_size = 1\n",
        "training_data = generate_training_data(window_size)\n"
      ],
      "metadata": {
        "id": "t1TWn6JkGyUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data:\")\n",
        "for pair in training_data:\n",
        "    print(pair)"
      ],
      "metadata": {
        "id": "cFk33W9IGyc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "xzUl737UGyfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's iterate over the training data and update the word vectors\n",
        "for target_word, context_word in training_data:\n",
        "    target_index = word_to_index[target_word]\n",
        "    context_index = word_to_index[context_word]\n",
        "\n",
        "    # Update the word vectors using the skip-gram update rule\n",
        "    word_vectors[target_index] -= learning_rate * word_vectors[context_index]"
      ],
      "metadata": {
        "id": "Bi6SdbE7GyiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word vectors after training:\")\n",
        "print(word_vectors)"
      ],
      "metadata": {
        "id": "O6yRYqraGyk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}