{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvyi0JTqKRH9uTVH79DpfT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Zipfs law** - describes the relationship between the frequency of a word in a corpus and its rank. The law states that the frequency of any given word is inversely proportional to its rank in the frequency table.\n",
        "\n",
        "Example Pokemon cards, We have 100 pokemon cards and we have to line them up from most common to least. According to the law, the frequency of a word is dependent on its rank. THerefore the most common card will appear the most and the second most common will appear half as many times, third common will be one third and so on."
      ],
      "metadata": {
        "id": "eMOX1Vde5xwO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_dm2Mbm5Llp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download the dataset of Medium articles from Hugging Face Hub\n",
        "df_articles = pd.read_csv(\n",
        "    hf_hub_download(\"fabiochiu/medium-articles\", repo_type=\"dataset\",\n",
        "                    filename=\"medium_articles.csv\")\n",
        ")\n",
        "\n",
        "\n",
        "df_articles = df_articles[:50000]\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "tokenized_words = df_articles[\"text\"].str.lower().str.split()\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_counts = {}\n",
        "for words in tokenized_words:\n",
        "    for word in words:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "# Sort the words by their frequencies in descending order\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "52faKuzCoSf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_words"
      ],
      "metadata": {
        "id": "k8SdAqnJoSlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**\n",
        "\n",
        "Term Frequency is like counting how many times a specific word appears in a dataset. Inverse Document Frequency is like thinking about how common or rare a word is across all corpus.\n",
        "\n",
        "Example TF-IDF helps us find words that are important in a specific book but not very common in all the books.\n",
        "\n",
        "1. Zipf's law explains the overall distribution of word frequencies in a corpus.\n",
        "2. TF-IDF focuses on measuring the importance of words within individual documents."
      ],
      "metadata": {
        "id": "giYiNw3uGlPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "YfpGOT-O211A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the TfidfVectorizer to the corpus\n",
        "corpus = df_articles[\"text\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "corpus_vectorized = vectorizer.fit_transform(corpus) # TF-IDF matrix for the corpus. it computes the TF-IDF score of each token with respect to every article. It transforms the corpus into a matrix where each row represents an article and each column represents a token.\n",
        "print(corpus_vectorized.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzUgMJhTPH01",
        "outputId": "16e1efdc-998c-4bde-9ab8-aeb2b1bbf3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 288140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize query\n",
        "query = \"natural language processing\"\n",
        "query_vectorized = vectorizer.transform([query]) #  TF-IDF vector for the query. it learns transformation to \"data science nlp\" without re-computing the parameters.\n",
        "print(query_vectorized.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs8DhpFfPH_A",
        "outputId": "9dbbf85d-27ac-417a-f9ae-ad0f5c463cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 288140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(corpus_vectorized.transpose()).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f11Vx_eETmJE",
        "outputId": "f55db815-81e2-41a9-a895-6bce45f219c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(288140, 50000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute scores as the dot product\n",
        "scores = query_vectorized.dot(corpus_vectorized.transpose()) # the query vector needs to be multiplied with each document vector in the corpus matrix. to measure the similarity of query to the each document in the corpus.\n",
        "scores_array = scores.toarray()[0]\n",
        "print(scores_array.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teI0hLgQPICD",
        "outputId": "5330019c-9f7f-44c4-eec4-96cf346d9e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the top_n articles with the highest scores and show them\n",
        "def show_best_results(df_articles, scores_array, top_n=10):\n",
        "  sorted_indices = scores_array.argsort()[::-1]\n",
        "  for position, idx in enumerate(sorted_indices[:top_n]):\n",
        "    row = df_articles.iloc[idx]\n",
        "    title = row[\"title\"]\n",
        "    score = scores_array[idx]\n",
        "    print(f\"{position + 1} [score = {score}]: {title}\")\n",
        "\n",
        "show_best_results(df_articles, scores_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15b0MlgbUV9s",
        "outputId": "d19a4e66-d605-4cc3-d42c-af5921db99d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 [score = 0.29307717929917204]: What the “Women in Language” Conference 2020 Taught Me\n",
            "2 [score = 0.2691499776299288]: Sentiment Analysis with Logistic Regression (Part 1)\n",
            "3 [score = 0.2682546479961668]: The Story of how Natural Language Processing is changing Financial Services in 2020\n",
            "4 [score = 0.2666674603843819]: List of free resources to learn Natural Language Processing\n",
            "5 [score = 0.2659237610875087]: To Expand Your Horizons, You Must Grow Your Language\n",
            "6 [score = 0.2538403402775664]: Popular Python Libraries in NLP: Dealing with Language Detection, Translation & Beyond!\n",
            "7 [score = 0.25119371362258425]: Natural Gas Is Dirtier than Coal\n",
            "8 [score = 0.24682913805000956]: How people wade through one of the top trending technology (artificial intelligence)\n",
            "9 [score = 0.2399478036375346]: On (Programming) Language Design\n",
            "10 [score = 0.2347842942649893]: What is Programming? How to get Started?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the dot is to measure the similarity between the query and each document in the corpus."
      ],
      "metadata": {
        "id": "SwvuRScKUbhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}